% % Template author: Mariana Rossi (2024) little chages Scheffler
% \documentclass[11pt]{article}
% \usepackage[sfdefault]{carlito} % Use Carlito, similar to Calibri
% \usepackage{geometry}
% \usepackage{setspace}
% \usepackage{titlesec}
% \usepackage{hyperref}
% \usepackage{graphicx}
% \usepackage{amsmath}
% \usepackage{bm}
% \usepackage{wrapfig}
% \usepackage{xcolor}
% \usepackage{authblk}
% \usepackage[font=small,labelfont=bf]{caption}


\newcommand{\MR}[1]{{\textcolor{purple}{\bf #1}}}

% % Set up the page dimensions
% \geometry{a4paper, margin=1in}

% % Customizing titles and section headers
% % Customizing titles and section headers
% \titleformat{\section}{\normalfont\fontsize{11}{13}\bfseries\sffamily}{\thesection}{1em}{}
% \titleformat{\subsection}{\normalfont\fontsize{11}{13}\bfseries\sffamily}{\thesubsection}{1em}{}
% \titleformat{\title}{\normalfont\fontsize{14}{16}\bfseries\sffamily}{}{0em}{}

% % Paragraph formatting: no indentation and slightly larger space between paragraphs
% \setlength{\parindent}{0pt}
% \setlength{\parskip}{0.5em}  % Adjust the space to your liking

 % ANNA: Beginn auf neuer Seite
\newpage

% % Document metadata
\section{Machine Learning the Electronic Structure in a Real-Space and Atom-Centered Framework}
\label{ChapML-ES}
\sectionauthor[1]{Joseph W. Abbott}
\sectionauthor[2]{Michele Ceriotti}
\sectionauthor[2]{Andrea Grisafi}
\sectionauthor[1]{Wei Bin How}
\sectionauthor[3]{Alan Lewis}
\sectionauthor[4]{Andrew J. Logsdail}
\sectionauthor[5]{Zekun Lou}
\sectionauthor[6,7]{Reinhard J. Maurer}
\sectionauthor[5]{\textbf{ *Mariana Rossi}}
\sectionauthor[4]{Pavel V. Stishenko}
\sectionauthor[8]{Zechen Tang}
\sectionlastauthor[8,9]{Yong Xu}

% \author[1]{Author Two}
% \author[2]{Author Three}

\sectionaffil[1]{Institute of Materials, \'Ecole Polytechnique F\'ed\'erale de Lausanne, Rte. Cantonale, Lausanne 1015, Switzerland}
\sectionaffil[2]{Physicochimie des \'Electrolytes et Nanosyst\`emes Interfaciaux, Sorbonne Universit\'e, CNRS, F-75005 Paris, France}
\sectionaffil[3]{Department of Chemistry, University of York, Heslington, York, YO10 5DD, UK}
\sectionaffil[4]{Cardiff Catalysis Institute, School of Chemistry, Cardiff University, Park Place, Cardiff CF10 3AT, United Kingdom}
\sectionaffil[5]{Max Planck Institute for the Structure and Dynamics of Matter, 22761 Hamburg, Germany}
\sectionaffil[6]{Department of Chemistry, University of Warwick, Gibbet Hill Road, CV4 7AL Coventry, United Kingdom}
\sectionaffil[7]{Department of Physics, University of Warwick, Gibbet Hill Road, CV4 7AL Coventry, United Kingdom}
\sectionaffil[8]{Department of Physics, Tsinghua University, Beijing 100084, P.R.China}
\sectionaffil[9]{RIKEN Center for Emergent Matter Science (CEMS), Wako, Saitama 351-0198, Japan}


\sectionaffil[*]{Coordinator of this contribution.}

% \affil[2]{University of Three}
% \date{}

% % Single line spacing
% \singlespacing

% \begin{document}

% \maketitle

\subsection*{Summary}

Machine-learning (ML) methods are driving a revolution in electronic structure simulations. 
Many methods target the learning and prediction of energies, forces and stresses that are produced by a density functional theory (DFT) calculation, with the goal of training high-accuracy force-fields that can be used to investigate atomic structural and dynamical properties of materials. 
However, one drawback of such models is that they do not give access to electronic-structure information, contrary to explicit quantum calculations that, in addition to energies and forces, also provide a wealth of derived electronic structure properties. 

A route to access these observables is to train machine learning models to learn a specific property, such as the electronic band gap, dipoles, surface work functions, and many others. 
However, a more universal solution exists in methods that directly target the electronic-density or the Kohn-Sham Hamiltonian (in the framework of DFT). 
When successful, these approaches are advantageous because, with a single ML model, one can in principle recover most electronic structure properties and, in principle, also calculate accurate energies and forces.

The field of ML for electronic structure is admittedly still in its infancy, despite a very fast pace of progress in recent years. Many different approaches have been reported in the literature, based on a diverse set of underlying ML models and basis representations. 
%Some examples based on local orbital basis representations include SchNOrb~\cite{schnorb}, DeepH~\cite{deeph,deephe3,xdeeph}, mlelec~\cite{nigam2022jcp,nigam2024mlelec},  ACEhamiltonians.jl~\cite{aceham}, HamGNN~\cite{hamgnn}, and SALTED~\cite{Lewis2021}. 
In this contribution, we focus on methods that are directly interfaced to FHI-aims, namely SALTED~\cite{Lewis2021}, rholearn~\cite{rholearn-abbott}, doslearn ~\cite{adaptive-how,rholearn-abbott}, ACEhamiltonians.jl~\cite{aceham, aceham_workflow} and DeepH~\cite{deeph,deephe3,xdeeph}. 

FHI-aims offers unique benefits for ML surrogate models of electronic structure. The numeric atomic-orbital (NAO) basis offers a compact, accurate, and local representation of the electronic structure and yields compact Hamiltonian, overlap, and density matrices. 
The seamless treatment of periodic and non-periodic systems means that molecules and solids can be treated with the same infrastructure, making the training of models easier and more versatile. 
%Both of these traits are advantageous for training surrogate models in a local orbital representation. 
Furthermore, the density fitting (resolution-of-identity) infrastructure that has been developed for hybrid functionals within the NAO basis~\cite{ren2012} provides the ingredients to represent various real-space quantities through a set of basis coefficients with known rotational symmetry properties. Finally, the Atomic Simulation Interface (ASI)~\cite{stishenko2023}, developed in FHI-aims, enables plain C and Python application programming interfaces, which are beneficial for integration with software packages and libraries as they allow in-memory data transfer with minimal latency and storage overheads, and with ensured data integrity.
%ML models and workflows to generate and manage data are increasingly available in \texttt{FHI-aims}. Data can be communicated inward and outward in raw or fitted forms, with the latter beneficial for reducing data quantities;  Data can also be communicated to and from \texttt{FHI-aims} \textit{via} file input/output (with minimal overheads), sockets-type interfaces, or through direct interfacing. Plain \texttt{C} and Python application programming interfaces of \texttt{FHI-aims} are beneficial for integration with software packages and libraries, as they allow in-memory data transfer with minimal latency and storage overheads and with ensured data integrity \cite{stishenko2023}. The approach to workflow development is therefore intimately linked to the ML problem under consideration.

%This forms the basis for the SALTED approach. & MC I think all this applies to every method so I wouldn't single out SALTED


\subsection*{Current Status of the Implementation}

\begin{figure}[ht]
    \centering
    %\includegraphics[width=0.6\textwidth]{salted_rholearn.png}
    \includegraphics[width=0.8\textwidth]{figures-ML-Electronic-Structure/MLStuff.pdf}
    \caption{A sketch of the ecosystem of machine-learning (ML) methods for the electronic structure that are interfaced to FHI-aims. The raw outputs of the code, the resolution of the identity (RI) representation of the electronic density, and the atomic simulation interface (ASI) enable diverse types of ML algorithms to be used.}
    \label{fig:ML-Method-Sketch}
\end{figure}
\vspace{0.3em}
Machine-learning methods that interface with FHI-aims and target the electronic structure are, so far, all based on the real-space representation of the Kohn-Sham problem or the density from a quantum mechanical calculation. The challenge of learning these quantities reduces to representing atom-centered local density contributions or intra- and inter-atomic matrix blocks. Because of the common representation of the electronic density $n(\bm{r})$ in isolated and periodic systems, FHI-aims offers a seamless route to learn $n(\bm{r})$ for mixed datasets. 
For the Hamiltonian of periodic systems, once the real-space Hamiltonian in the extended crystal volume is known, the Hamiltonian at arbitrary $k$-points can be evaluated in a straightforward manner, similar to tight-binding techniques.

We start by summarizing methods that target $n(\bm{r})$. SALTED is a machine learning method based on a symmetry-adapted Gaussian process regression.\cite{Lewis2021} Using a resolution-of-the-identity (RI) expansion, $n(\bm{r})$ (from LDA, GGA, or hybrid-functionals) is first expressed as a linear combination of atom-centred basis functions $\phi_{i\sigma}$:
\begin{equation}
    n(\bm{r}) \approx \tilde{n}(\bm{r}) = \sum_{i,\sigma,\mathbf{U}} c_{i\sigma} \phi_{i\sigma}(\bm{r} - \bm{R}_i - \bm{T}(\mathbf{U}))
\end{equation}
where $i$ labels the atom on which the basis function is centered, $\bm{T}$ is a translation vector, and $\sigma=(n\lambda\mu)$ is a compound label describing the radial $R_n$ and spherical $Y_{\lambda\mu}$ part of the basis function. In FHI-aims, these basis functions can be chosen from the auxiliary basis introduced to accelerate the calculation of hybrid functionals.\cite{ren2012} The RI expansion coefficients $c_{i\sigma}\equiv c_\sigma(A_i)$ are then themselves approximated as a linear combination of a similarity measure $k_{\mu \mu^\prime}^\lambda$ between the atomic environment associated with the target coefficient $A_i$ and each reference atomic environment $M_j$, and their associated regression weights $b_{\mu j}^\lambda$ to be determined using SALTED:
\begin{equation}
     c_\sigma(A_i) = \sum_j \sum_{\mu^\prime \leq \lambda} b_{\mu j}^\lambda k_{\mu \mu^\prime}^\lambda (A_i, M_j).
\end{equation}
These similarity measures are symmetry adapted, being tensors of dimension $2\lambda+1$ that transform as the $\lambda$-th irrep of the O(3) group, since the basis functions and their associated coefficients transform in this way. In SALTED, atom-density based descriptors are computed with the rascaline~\cite{rascaline} library. These can be  smooth-overlap of atomic position (SOAP) descriptors, or descriptors that contain information about long-range electrostatics (\textit{e.g.}, LODE descriptors).

For a full description of the calculation of the regression weight-vector $\bm{b}$, which lies at the core of the symmetry-adapted Gaussian process regression of the SALTED method, the reader is referred to Refs.~\cite{Lewis2021, salted-grisafi}. Here we highlight that, in order to calculate $\mathbf{b}$, two quantities are required for each structure in the training set: $\mathbf{S}$, the overlap matrix of the basis functions $\phi_i$; and the optimal expansion coefficients $\mathbf{c}^\textrm{RI} = \mathbf{S}^{-1}\mathbf{w}$, where $\mathbf{w}$ is a vector containing the projection of the self-consistent electron density onto each basis function $\phi_{i\sigma}$.\footnote{The weights are 
$
    w_{i\sigma} = \sum^{\mathbf{U}_\text{cut}}_\mathbf{U}\braket{\phi_{i\sigma}(\mathbf{U})}{n}_\text{u.c.}.$
 The subscript ``u.c.'' indicates that in FHI-aims the integral is performed only over the ``central'' unit cell in the case of periodic systems, with the sum over $\mathbf{U}$ being truncated to only include unit cells that contain basis functions with some support in this central unit cell. The overlap matrix $\textbf{S}$ is evaluated in a similar way.}  

The calculation of $\mathbf{S}$, $\mathbf{w}$ and 
$\mathbf{c}^{\text{RI}}$ has been implemented in FHI-aims, with the option to write these quantities to files for later use with SALTED to train a ML model. Additionally, SALTED has a number of FHI-aims specific ``helper'' functions to support integration between the two codes. These include functions to calculate the error introduced by the RI approximation, to help users find an accurate set of basis functions $\phi_{i\sigma}$, to convert the data output from FHI-aims into a suitable format to be read by SALTED, and to convert coefficients predicted using SALTED into a format that can be read by FHI-aims. In the latter case, FHI-aims has the capability to read in a set of expansion coefficients $c_{i\sigma}$ during the SCF initialisation, and construct from them $n(\bm{r})$ on the real-space integration grid used in the code. This approach replaces the default choice of the initial electron density (\textit{i.e.}, superposition of atomic densities). In preliminary tests with a SALTED model trained on a subset of around 9000 structures of the QM9 dataset~\cite{qm9} (see Ref.~\cite{salted-grisafi}) we have observed, on average, that this approach leads to a $35$\% reduction in the number of SCF steps needed for convergence.
%to start FHI-aims from a learned density with the PBE functional. For 500 structures of the same set, we logged the number of SCF cycles until convergence when starting from a predicted SALTED density and from the usual superposition of free atom densities, with the same default convergence settings. The results, which represent a proof of concept of what can be gained by such procedure, are reported in Fig.~\ref{fig:scf-gain}.
% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.5\textwidth]{salted-gain-draft.png}
%     \caption{Speed up in SCF convergence for 500 randomly selected structures of the QM9 dataset. The speedup factor is calculated with respect to the number of SCF cycles of the same calculation starting from the standard initial free atom superposition density.}
%     \label{fig:scf-gain}
% \end{figure}

The infrastructure built for SALTED in FHI-aims, including the  RI representation for $n(\bm{r})$, can be re-used for ML schemes that use a neural network (NN) architecture rather than kernel regression. One such method is rholearn~\cite{rholearn-abbott}.  While the inputs (nuclear coordinates) and outputs (RI coefficients) of the prediction pipeline are the same, there are a few key differences compared to SALTED. %that can be attributed to the modular software ecosystem upon which \texttt{rholearn} is built. 
The rascaline~\cite{rascaline} library  is used to generate equivariant descriptors of arbitrary body order through Clebsch-Gordan products. These descriptor \textit{vectors}, as opposed to the kernels used in SALTED, form the part of the typically (but not necessarily) fixed transformations of the nuclear coordinates into an intermediate representation. Then, the training workflow uses the metatensor-torch library as its backbone. Native torch modules for tensor operations and learning utilities are wrapped to be compatible with TensorMap, the sparse data storage format of the metatensor~\cite{metatensor} library. This allows arbitrarily complex equivariance-preserving NNs to be defined, and applied to the equivariant descriptors. Finally, the use of descriptor vectors, the ability to train by minibatch gradient descent, and the use of sparse operations (most notably when evaluating the loss) reduce the memory overhead of rholearn. 

Models that target learning the (Kohn-Sham) Hamiltonian matrix profit from a  more tightly integrated interface with FHI-aims for AI/ML workflows. This interface is realised via ASI \cite{stishenko2023}, which is a plain C interface that provides direct in-memory access to electronic structure quantities within FHI-aims, such as the Hamiltonian, and overlap matrices. It also allows to inject AI/ML predicted matrices into FHI-aims processes at  runtime, providing new opportunities in modular software integration \cite{aceham_workflow, stishenko2025practical}. The integration of ACEhamiltonians.jl and DeepH to FHI-aims use this interface.

ACEhamiltonians.jl~\cite{aceham} uses atom- and bond-centred descriptors based on the Atomic Cluster Expansion (ACE)~\cite{ace-descs} to represent non-orthogonal Hamiltonian matrix blocks that transform equivariantly with respect to the full rotation group. ACEhamiltonians models are analytical and linear, and can produce accurate models with little data. The model can directly be trained on FHI-aims data that is outputted either \textit{via} the keyword \texttt{output\_rs\_matrices} in human readable or HDF5 format, or through the ASI interface. Furthermore, the model is also integrated with FHI-aims through ASI to directly feed predicted quantities such as the Hamiltonian or the density matrix back into FHI-aims, \textit{e.g.}, to provide an improved initial guess of the density matrix for the SCF algorithm. We have also shown that ASI can be used as a bridge to train ACEhamiltonians models on FHI-aims data and then inject them into a different electronic structure code, such as DFTB+, during runtime.~\cite{aceham_workflow}

% It has been finalized by Prof. Xu and Z. Tang.
The deep-learning density functional theory Hamiltonian (DeepH) method~\cite{deeph,deephe3,xdeeph} is an equivariant graph neural network approach for learning and predicting the electronic Hamiltonians for given material structures. Based on the predicted Hamiltonian, various physical properties can also be derived, including band structures, density of states, and the electric susceptibility, among others. Benefiting from the principle of nearsightedness, the neural networks of DeepH can infer the properties of large-scale material structures by learning from small ones. In the original version of DeepH developed in 2021~\cite{deeph}, a local-coordinates-based scheme was proposed to handle the rotational covariance of the DFT Hamiltonian matrix. A subsequent implementation, DeepH-E3~\cite{deephe3}, incorporated an E(3)-equivariant neural network architecture to address the equivariance with respect to the E(3) group, vastly improving the prediction accuracy. Recently, more advanced equivariant tensor product techniques together with transformer architectures have been integrated into the DeepH framework, further enhancing both accuracy and efficiency~\cite{deeph2}. The method is versatile and has also been extended to various deep-learning electronic structure schemes, including magnetic materials~\cite{xdeeph}, density-functional perturbation theory~\cite{deeph-dfpt}, hybrid density functionals~\cite{deeph-hybrid}, among others. The current implementation of the DeepH interface for FHI-aims is based on the ASI package~\cite{stishenko2023}. Structural information, Hamiltonian matrices and overlap matrices are extracted from the ASI plug-ins and converted to DeepH formats. The current interface has been tested on both molecular and periodic systems, in which the open source DeepH-E3 method~\cite{deephe3} reaches sub-meV accuracy in terms of Hamiltonian matrix elements across the test systems.

Finally, there are ML methods interfaced with FHI-aims that parse raw outputs of electronic-structure quantities from FHI-aims. doslearn targets the electronic density of states (DOS), employing a locality ansatz in which the total DOS of a structure is built as a sum of atom-centered contributions~\cite{ben_mahmoud2020}. The machine learning pipeline directly takes in FHI-aims outputs, extracts the Kohn-Sham eigenvalues for each structure, and uses scipy to generate cubic Hermite splines of the DOS to support an adaptive energy reference, necessary for  bulk periodic calculations. Model training is performed using the PyTorch package, with the adaptive reference framework supported \textit{via} a modification of the loss function to concurrently optimize the energy reference during model training. Although the current doslearn implementation employs a simple feed forward network using invariant SOAP Power Spectrum descriptors generated from the rascaline~\cite{rascaline} library as inputs, doslearn primarily focuses on the optimization procedure and is model agnostic, allowing it to be easily modified to deploy on any arbitrary model architecture of the user's choice.


\subsection*{Usability and Tutorials}

The usability of the described electronic-structure learning techniques with FHI-aims is ensured by specific keywords, with examples and tutorials maintained either natively in the code or in dedicated repositories.

We begin by briefly describing the usability and tutorials of the electronic-density learning approaches.  The SALTED code is written in Python and FORTRAN and it is available through a dedicated GitHub repository\footnote{\href{https://github.com/andreagrisafi/SALTED}{\texttt{https://github.com/andreagrisafi/SALTED}}}. \textit{rholearn} is Python-based and is also available through a dedicated GitHub repository\footnote{\href{https://github.com/lab-cosmo/rholearn}{\texttt{https://github.com/lab-cosmo/rholearn}}}. Their installation and dependencies are described in their respective documentation. 

Briefly, the FHI-aims keywords for \texttt{control.in} that trigger the outputs needed for SALTED and rholearn are:
\begin{itemize}
    \item \texttt{ri\_density\_restart write} will create a file in the working directory containing the coefficients $\textbf{c}^{\textrm{RI}}$.
    \item \texttt{ri\_density\_restart read} will read the expansion coefficients contained in the file previously created (in the same directory) and replace the default initial density with the density obtained from these coefficients.
    \item \texttt{ri\_full\_output}, when used in conjunction with \texttt{ri\_density\_restart write}, will additionally output a file containing the overlap matrix $\textbf{S}$, and the file \texttt{ri\_projections.out} containing the projections vector $\textbf{w}$.
    \item \texttt{ri\_skip\_scf} is used in conjunction with \texttt{ri\_density\_restart write} and allows RI decomposition of a previously-converged density as part of a two-step SCF + RI process. For instance, \texttt{elsi\_restart write} can be used to save a converged solution to the SCF procedure, then \texttt{elsi\_restart read} and \texttt{ri\_skip\_scf} can be used to fit directly to the corresponding converged density.
\end{itemize}

The keywords \texttt{prodbas\_acc}, \texttt{max\_l\_prodbas}, and \texttt{wave\_threshold} are not SALTED or rholearn-specific, but control the construction of the auxiliary basis that is used to expand the density. Specifying these keywords ensures a consistent basis is used, which is especially important if training on mixed periodic and non-periodic datasets.

The SALTED integration with FHI-aims is thoroughly described in an online tutorial\footnote{\href{https://fhi-aims-club.gitlab.io/tutorials/fhi-aims-with-salted}{\texttt{https://fhi-aims-club.gitlab.io/tutorials/fhi-aims-with-salted}}}. The tutorial guides the user on training a SALTED model for the electronic density of a water monomer, and predicting the density of water dimers. The rholearn integration with FHI-aims shares many of the features of the SALTED integration and is also described in an online tutorial\footnote{\href{https://github.com/lab-cosmo/rholearn/tree/main/example/rholearn-aims-tutorial}{\texttt{https://github.com/lab-cosmo/rholearn/tree/main/example/rholearn-aims-tutorial}}}. Data for a subset of the QM7 dataset~\cite{qm7-1,qm7-2} is generated with the \texttt{aims-interface} in rholearn, and a descriptor-based equivariant neural network trained by minibatch gradient descent using a PyTorch- and metatensor~\cite{metatensor}-based workflow.

The interfaces for Hamiltonian learning are based on ASI, as mentioned above.
The ASI documentation is available \textit{via} the library website \cite{asi2024}. The Python wrapper for ASI, asi4py, can be installed from the Python Package Index (PyPI) repository. Tutorials on how to train ACEhamiltonians models with example FHI-aims data for bulk Aluminium are provided on GitHub~\cite{aceham-web}. The tutorial covers how to parse and process FHI-aims Hamiltonian data generated with the \texttt{output\_rs\_matrices} keyword into the expected HDF5 database structure. It also covers essential keywords and hyperparameters for model construction, model fitting, and model prediction. ACEhamiltonians directly predicts the real-space representation of the Hamiltonian in the extended crystal volume. Matrices (and therefore eigenvalues and eigenvectors) during prediction can be constructed at arbitrary $\mathbf{k}$-points. 

Documentation on how to use ASI to provide an improved initialisation of the density matrix in FHI-aims during runtime is available in the \texttt{dm} submodule of asi4py\footnote{\href{https://pvst.gitlab.io/asi/ml\_dm.html}{\texttt{https://pvst.gitlab.io/asi/ml\_dm.html}}}. This submodule provides a prediction of the density matrix that can reduce the number of required self-consistent-field iterations, when compared to the standard initialization ~\cite{stishenko2025practical}.  The \texttt{ASI\_register\_dm\_init\_callback} function of the ASI API is used to register a user-provided callback function that returns an initial guess of the density matrix at the beginning of the SCF loop. The \texttt{asi4py.dm} module provides a few classes implementing density matrix prediction via ASI; these classes can be used directly, subclassed, or used as inspiration for custom density matrix predictors.

A tutorial for DeepH is available in Zenodo~\cite{aims2deeph-zenodo}, which provides the source code for the interface with FHI-aims, along with demonstrations for generating datasets and two example datasets, each accompanied by configuration files for DeepH-E3 training. The source code for the interface is also distributed alongside FHI-aims. To utilize the interface, FHI-aims should be compiled as a shared library through ASI and the asi4py package should also be installed. The interface can be employed to extract all required data for DeepH training, utilizing the ASI API to access Hamiltonian and overlap matrices. The DeepH-E3 model~\cite{deephe3} has been evaluated on water and graphene datasets provided in the Zenodo repository, achieving mean average errors of 0.31 and 0.28~meV, respectively, for the Hamiltonian matrix elements in the test data sets. The recently developed version DeepH-2~\cite{deeph2}, which will be soon released, is expected to show higher prediction accuracy than DeepH-E3.

Finally, the doslearn integration with FHI-aims is available in the the same repository\footnote{\href{https://github.com/lab-cosmo/rholearn}{\texttt{https://github.com/lab-cosmo/rholearn}}} as rholearn, with an \href{https://github.com/lab-cosmo/rholearn/tree/main/example/doslearn-aims-tutorial}{an online tutorial} that covers the data processing pipeline and model training of a fully connected network using batch gradient descent. The keyword used to generate the energy eigenvalues necessary for doslearn is: \texttt{output postscf\_eigenvalues}.

% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=\textwidth]{placeholder.pdf}
%     \caption{some caption.}
%     \label{fig:placeholder}
% \end{figure}


\subsection*{Future Plans and Challenges}

%% This needs a future vision contribution from authors on AI/ML projects

The area of ML applied directly to the electronic-structure is growing at a fast pace. Many such methods, including the ones interfaced with FHI-aims discussed in this contribution, show high-accuracy in predicting the electronic-density and Hamiltonian matrix elements. The methods allow the prediction of electronic structure properties of systems containing up to $\sim$10$^5$ atoms when trained on much smaller models. Efficiency in terms of speed and memory usage can still be bottlenecks, and these are currently being addressed by the community. It is exciting to see that such methods are already able to predict not only the ground-state electronic density and Hamiltonian, but also density linear-responses~\cite{lewi+jcp2023, grisafi-prm-2023, deeph-dfpt}, for example. Remaining challenges in this area regard the proper and efficient handling of equivariant architectures for vector-field prediction. 

It is further interesting to see the extension of methods such as DeepH to unsupervised learning~\cite{deeph-zero}, force fields for magnetic materials~\cite{magnet}, and to train high-accuracy universal models covering over 20 elements in the periodic table and over 10,000 material structures~\cite{deeph-umm}.
For DeepH, despite its $O(N)$ scaling that enables highly efficient computation of large systems, there remain a few post-processing workflows for calculating physical properties that could be computationally expensive for studying large-size material systems. Future advances in efficient algorithms for property calculations, such as sparse-matrix techniques, Green’s function methods, and Wannierization, will significantly enhance the practical applications of deep-learning electronic structure methods, allowing for the computation of a broader range of physical properties in large-scale material simulations.

Finally, to realise the full potential of ML for electronic structure, new infrastructure is necessary to integrate existing software with AI-enabling workflows. 
ML models and workflows to generate and manage data are increasingly available in FHI-aims. Data can be communicated inward and outward in raw or fitted forms, with the latter beneficial for reducing data quantities;  Data can also be communicated to and from FHI-aims \textit{via} file input/output (with minimal overheads), sockets-type interfaces, or through direct interfacing.
Nevertheless, there is an ever growing need for flexible and robust interfaces that allow more user interaction with these calculations, and data extraction. This need motivates further efforts to support workflow connectivity and development of standardised data communication paradigms. For the direct access to in-memory data objects, development plans for ASI include more flexible and extendable interface that supports a wider range of data objects and diverse sparse matrix formats. Long-term aims include applying ASI to develop an infrastructure for integration with a broad range of ML models, which can then accelerate or refine electronic structure calculations.


\subsection*{Acknowledgements}

We acknowledge fruitful discussions with Volker Blum, Ben Hourahine, Thomas Keal, and Scott Woodley. We thank Adam McSloy and Connor L. Box for their contributions to Hamiltonian output functionality in FHI-aims.  

AJL and PVS acknowledge funding by the UKRI Future Leaders Fellowship program (MR/T018372/1, MR/Y034279/1). 
AJL, PVS, RJM, and MR acknowledge funding from the ARCHER2 eCSE Programme (eCSE03-10).
RJM acknowledges support through the UKRI Future Leaders Fellowship programme (MR/S016023/1, MR/X023109/1), and a UKRI frontier research grant (EP/X014088/1).
YX and ZT acknowledge funding from the Basic Science Center Project of NSFC (52388201), the Ministry of Science and Technology of China (2023YFA1406400), the National Natural Science Foundation of China (12334003, 12421004, and 12361141826), and the National Science Fund for Distinguished Young Scholars (12025405).


% We acknowledge ...

% % Explicit bibliography to make it easier to format in the future
% \begin{thebibliography}{10}

% \bibitem{baro+rmp2001}
% S. Baroni, S. de Gironcoli, A. D. Corso, P. Giannozzi, \textit{Phonons and related crystal properties from density-functional perturbation theory}. Rev. Mod. Phys. \textbf{73}, 515–562 (2001)

% \bibitem{gian+prb1991}
% P. Giannozzi, S. de Gironcoli, P. Pavone, S. Baroni, \textit{Ab initio calculation of phonon dispersions in semiconductors}. Phys. Rev. B \textbf{43}, 7231-7242 (1991)
  
% \end{thebibliography}


% \end{document}
